{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411062e0",
   "metadata": {},
   "source": [
    "# Cell 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20738f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: matplotlib in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: statsmodels in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (0.14.4)\n",
      "Requirement already satisfied: openpyxl in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from matplotlib) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from statsmodels) (1.15.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: et-xmlfile in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\time-series-forecasting\\time-series-yash\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy matplotlib seaborn statsmodels openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb976ff",
   "metadata": {},
   "source": [
    "# Cell 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dfd817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88c716",
   "metadata": {},
   "source": [
    "# Cell 3: Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62492103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_data(file_path):\n",
    "    \"\"\"Load Excel file with multiple sheets\"\"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        sheets = {sheet_name: xls.parse(sheet_name) for sheet_name in xls.sheet_names}\n",
    "        print(f\"Successfully loaded {len(sheets)} sheets: {list(sheets.keys())}\")\n",
    "        return sheets\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Excel file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49646a0a",
   "metadata": {},
   "source": [
    "# Cell 4: Column Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e054276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    \"\"\"Clean and standardize column names\"\"\"\n",
    "    original_cols = df.columns.tolist()\n",
    "    df.columns = [re.sub(r'[^a-z0-9_]', '', col.lower().replace(' ', '_')) for col in df.columns]\n",
    "    print(f\"Cleaned {len(df.columns)} column names\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df667d67",
   "metadata": {},
   "source": [
    "# Cell 5: Date Column Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b124119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_columns(df):\n",
    "    \"\"\"Convert date columns (e.g., Jan-23) to numeric\"\"\"\n",
    "    date_cols = [col for col in df.columns if re.match(r'^\\w{3}-\\d{2}$', col)]\n",
    "    print(f\"Found {len(date_cols)} date columns: {date_cols[:5]}...\" if len(date_cols) > 5 else f\"Found {len(date_cols)} date columns: {date_cols}\")\n",
    "    \n",
    "    for col in date_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c84afb",
   "metadata": {},
   "source": [
    "# Cell 6: Missing Values Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2774c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"Handle missing values with interpolation or zero-filling\"\"\"\n",
    "    sales_cols = [col for col in df.columns if re.match(r'^\\w{3}-\\d{2}$|^202\\d$|^total_qty$', col)]\n",
    "    \n",
    "    # Calculate non-zero ratio for each product\n",
    "    non_zero_ratio = (df[sales_cols] != 0).sum(axis=1) / len(sales_cols)\n",
    "    consistent_products = non_zero_ratio > 0.5\n",
    "    \n",
    "    print(f\"Found {consistent_products.sum()} consistent products (>50% non-zero sales)\")\n",
    "    \n",
    "    # Apply interpolation for consistent products\n",
    "    for idx in df[consistent_products].index:\n",
    "        df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    # Fill remaining NaNs with 0\n",
    "    df[sales_cols] = df[sales_cols].fillna(0)\n",
    "    \n",
    "    # Remove products with all zero sales\n",
    "    initial_rows = len(df)\n",
    "    df = df[df[sales_cols].sum(axis=1) > 0]\n",
    "    print(f\"Removed {initial_rows - len(df)} products with all zero sales\")\n",
    "    \n",
    "    # Handle string columns\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    df[string_cols] = df[string_cols].fillna('Unknown')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e7775b",
   "metadata": {},
   "source": [
    "# Cell 7: Outlier Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5d5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_handle_outliers_stl(df, period=12, threshold=3, method='flag'):\n",
    "    \"\"\"Detect outliers using STL decomposition\"\"\"\n",
    "    sales_cols = [col for col in df.columns if re.match(r'^\\w{3}-\\d{2}$')]\n",
    "    outlier_flags = pd.DataFrame(0, index=df.index, columns=[f'{col}_outlier' for col in sales_cols])\n",
    "    outlier_count = 0\n",
    "    \n",
    "    print(f\"Analyzing {len(df)} products for outliers...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        sales = row[sales_cols].values\n",
    "        if np.all(sales == 0) or len(sales) < period:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            stl = STL(sales, period=period, robust=True).fit()\n",
    "            residuals = stl.resid\n",
    "            z_scores = (residuals - np.mean(residuals)) / np.std(residuals)\n",
    "            outliers = np.abs(z_scores) > threshold\n",
    "            \n",
    "            # Flag outliers\n",
    "            for col, outlier in zip(sales_cols, outliers):\n",
    "                outlier_flags.at[idx, f'{col}_outlier'] = int(outlier)\n",
    "            \n",
    "            # Cap outliers if method is 'cap'\n",
    "            if method == 'cap':\n",
    "                trend = stl.trend + stl.seasonal\n",
    "                lower_bound = trend - threshold * np.std(residuals)\n",
    "                upper_bound = trend + threshold * np.std(residuals)\n",
    "                df.loc[idx, sales_cols] = np.clip(sales, lower_bound, upper_bound)\n",
    "            \n",
    "            if outliers.any():\n",
    "                outlier_count += 1\n",
    "                if outlier_count <= 5:  # Show first 5 examples\n",
    "                    print(f\"Outliers for product {row['product_id']}: {outliers.sum()} cases\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Found outliers in {outlier_count} products\")\n",
    "    df = pd.concat([df, outlier_flags], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715dda14",
   "metadata": {},
   "source": [
    "# Cell 8: Smoothing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1daed7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smoothing(df, window=3, min_variance=1000):\n",
    "    \"\"\"Apply moving average smoothing to noisy products\"\"\"\n",
    "    sales_cols = [col for col in df.columns if re.match(r'^\\w{3}-\\d{2}$')]\n",
    "    smoothed_count = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        sales = row[sales_cols].values\n",
    "        if np.var(sales) > min_variance:\n",
    "            smoothed = pd.Series(sales).rolling(window=window, center=True, min_periods=1).mean()\n",
    "            df.loc[idx, sales_cols] = smoothed\n",
    "            smoothed_count += 1\n",
    "    \n",
    "    print(f\"Applied smoothing to {smoothed_count} products with high variance\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd6666",
   "metadata": {},
   "source": [
    "# Cell 9: Rule-based Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c759de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_validation(df, max_threshold=10):\n",
    "    \"\"\"Apply rule-based validation for negative and unrealistic sales\"\"\"\n",
    "    sales_cols = [col for col in df.columns if re.match(r'^\\w{3}-\\d{2}$')]\n",
    "    validation_flags = pd.DataFrame(0, index=df.index, columns=[f'{col}_invalid' for col in sales_cols])\n",
    "    invalid_count = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        sales = row[sales_cols]\n",
    "        \n",
    "        # Check for negative sales\n",
    "        negative_sales = sales < 0\n",
    "        for col in sales_cols[negative_sales]:\n",
    "            validation_flags.at[idx, f'{col}_invalid'] = 1\n",
    "            df.at[idx, col] = 0\n",
    "            invalid_count += 1\n",
    "        \n",
    "        # Check for unrealistic sales (10x historical max)\n",
    "        historical_max = sales.max()\n",
    "        if historical_max > 0:\n",
    "            unrealistic_sales = sales > historical_max * max_threshold\n",
    "            for col in sales_cols[unrealistic_sales]:\n",
    "                validation_flags.at[idx, f'{col}_invalid'] = 1\n",
    "                invalid_count += 1\n",
    "    \n",
    "    print(f\"Fixed {invalid_count} invalid sales values\")\n",
    "    df = pd.concat([df, validation_flags], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b73215",
   "metadata": {},
   "source": [
    "# Cell 10: Zero Values Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb00a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_zero_values(df, sheet_name):\n",
    "    \"\"\"Check and report zero values in sales columns\"\"\"\n",
    "    sales_cols = [col for col in df.columns if re.match(r'^\\w{3}-\\d{2}$|^202\\d$|^total_qty$', col)]\n",
    "    zero_counts = (df[sales_cols] == 0).sum()\n",
    "    total_zeros = zero_counts.sum()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Zero value check for sheet '{sheet_name}':\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if total_zeros == 0:\n",
    "        print(\"✅ No zero values found in sales columns.\")\n",
    "    else:\n",
    "        print(f\"⚠️  Found {total_zeros} zero values across sales columns:\")\n",
    "        for col, count in zero_counts.items():\n",
    "            if count > 0:\n",
    "                print(f\"   - {col}: {count} zeros\")\n",
    "    \n",
    "    return total_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21176b4e",
   "metadata": {},
   "source": [
    "# Cell 11: Yearly Totals Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed5b4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_yearly_totals(df):\n",
    "    \"\"\"Validate yearly totals against monthly sums\"\"\"\n",
    "    year_cols = [col for col in df.columns if re.match(r'^202\\d$', col)]\n",
    "    monthly_cols = [col for col in df.columns if re.match(r'^\\w{3}-\\d{2}$', col)]\n",
    "    \n",
    "    print(f\"Validating {len(year_cols)} yearly columns against monthly data\")\n",
    "    \n",
    "    for year in year_cols:\n",
    "        year_num = int(year)\n",
    "        relevant_months = [col for col in monthly_cols if f\"{year_num % 100:02d}\" in col]\n",
    "        \n",
    "        if relevant_months:\n",
    "            df[f'{year}_calc'] = df[relevant_months].sum(axis=1)\n",
    "            df[f'{year}_discrepancy'] = df[year] - df[f'{year}_calc']\n",
    "            df[f'{year}_flag'] = df[f'{year}_discrepancy'].abs() > 0.01  # Allow small rounding differences\n",
    "            \n",
    "            discrepancies = df[f'{year}_flag'].sum()\n",
    "            if discrepancies > 0:\n",
    "                print(f\"   - {year}: {discrepancies} products have discrepancies\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989effb7",
   "metadata": {},
   "source": [
    "# Cell 12: Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "374e0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_outliers(df, product_id, sheet_name):\n",
    "    \"\"\"Visualize sales trends for a specific product\"\"\"\n",
    "    product_data = df[df['product_id'] == product_id]\n",
    "    if product_data.empty:\n",
    "        print(f\"❌ No data found for product {product_id}\")\n",
    "        return\n",
    "    \n",
    "    sales_cols = [col for col in df.columns if re.match(r'^\\w{3}-\\d{2}$', col)]\n",
    "    sales_data = product_data[sales_cols].iloc[0]\n",
    "    \n",
    "    # Create time series\n",
    "    try:\n",
    "        dates = pd.to_datetime(sales_cols, format='%b-%y')\n",
    "        sales_df = pd.DataFrame({'Date': dates, 'Sales': sales_data.values})\n",
    "        sales_df = sales_df.sort_values('Date')\n",
    "        \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(sales_df['Date'], sales_df['Sales'], marker='o', linewidth=2, markersize=6)\n",
    "        plt.title(f'Sales Trend for Product {product_id} ({sheet_name})', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Sales Quantity')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add summary statistics\n",
    "        plt.subplot(2, 1, 2)\n",
    "        stats_text = f\"\"\"\n",
    "        Product: {product_id}\n",
    "        Total Sales: {sales_data.sum():,.0f}\n",
    "        Average Monthly Sales: {sales_data.mean():.1f}\n",
    "        Max Monthly Sales: {sales_data.max():,.0f}\n",
    "        Min Monthly Sales: {sales_data.min():,.0f}\n",
    "        Months with Zero Sales: {(sales_data == 0).sum()}\n",
    "        \"\"\"\n",
    "        plt.text(0.1, 0.5, stats_text, transform=plt.gca().transAxes, fontsize=11,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating visualization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b6efe",
   "metadata": {},
   "source": [
    "# Cell 13: Main Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e37bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sales_data(df, sheet_name):\n",
    "    \"\"\"Main cleaning pipeline for sales data\"\"\"\n",
    "    print(f\"\\n🔄 Starting cleaning pipeline for sheet: '{sheet_name}'\")\n",
    "    print(f\"Initial data shape: {df.shape}\")\n",
    "    \n",
    "    # Step 1: Clean column names\n",
    "    print(\"\\n1️⃣ Cleaning column names...\")\n",
    "    df = clean_column_names(df)\n",
    "    \n",
    "    # Step 2: Convert date columns\n",
    "    print(\"\\n2️⃣ Converting date columns...\")\n",
    "    df = convert_date_columns(df)\n",
    "    \n",
    "    # Step 3: Handle missing values\n",
    "    print(\"\\n3️⃣ Handling missing values...\")\n",
    "    df = handle_missing_values(df)\n",
    "    \n",
    "    # Step 4: Apply rule-based validation\n",
    "    print(\"\\n4️⃣ Applying rule-based validation...\")\n",
    "    df = rule_based_validation(df)\n",
    "    \n",
    "    # Step 5: Detect outliers\n",
    "    print(\"\\n5️⃣ Detecting outliers...\")\n",
    "    df = detect_and_handle_outliers_stl(df, method='flag')\n",
    "    \n",
    "    # Step 6: Apply smoothing\n",
    "    print(\"\\n6️⃣ Applying smoothing...\")\n",
    "    df = apply_smoothing(df)\n",
    "    \n",
    "    # Step 7: Validate yearly totals\n",
    "    print(\"\\n7️⃣ Validating yearly totals...\")\n",
    "    df = validate_yearly_totals(df)\n",
    "    \n",
    "    # Step 8: Ensure data types\n",
    "    print(\"\\n8️⃣ Ensuring proper data types...\")\n",
    "    numeric_cols = [col for col in df.columns if re.match(r'^\\w{3}-\\d{2}$|^202\\d$|^total_qty$', col)]\n",
    "    df[numeric_cols] = df[numeric_cols].astype(float)\n",
    "    \n",
    "    # Step 9: Remove duplicates\n",
    "    print(\"\\n9️⃣ Removing duplicates...\")\n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates(subset=['product_id'], keep='first')\n",
    "    duplicates_removed = initial_rows - len(df)\n",
    "    print(f\"Removed {duplicates_removed} duplicate rows based on Product ID\")\n",
    "    \n",
    "    # Step 10: Standardize IDs\n",
    "    print(\"\\n🔟 Standardizing IDs...\")\n",
    "    if 'customer_id_sold_to' in df.columns:\n",
    "        df['customer_id_sold_to'] = df['customer_id_sold_to'].astype(str).str.strip()\n",
    "    df['product_id'] = df['product_id'].astype(str).str.strip()\n",
    "    \n",
    "    # Step 11: Check zero values\n",
    "    check_zero_values(df, sheet_name)\n",
    "    \n",
    "    # Step 12: Create sample visualization\n",
    "    print(f\"\\n📊 Creating sample visualization...\")\n",
    "    if not df.empty and len(df) > 0:\n",
    "        sample_product = df['product_id'].iloc[0]\n",
    "        visualize_outliers(df, sample_product, sheet_name)\n",
    "    \n",
    "    print(f\"\\n✅ Cleaning completed! Final data shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18a1be",
   "metadata": {},
   "source": [
    "# Cell 14: Main Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebad0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file_path='Sales History.xlsx', output_path='Cleaned_Sales_History_Enhanced_Updated.xlsx'):\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"🚀 Starting Sales Data Cleaning Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"📂 Loading Excel data...\")\n",
    "    sheets = load_excel_data(file_path)\n",
    "    if not sheets:\n",
    "        print(\"❌ Failed to load data. Please check the file path.\")\n",
    "        return\n",
    "    \n",
    "    # Clean each sheet\n",
    "    cleaned_sheets = {}\n",
    "    for sheet_name, df in sheets.items():\n",
    "        cleaned_df = clean_sales_data(df, sheet_name)\n",
    "        cleaned_sheets[sheet_name] = cleaned_df\n",
    "    \n",
    "    # Save cleaned data\n",
    "    print(f\"\\n💾 Saving cleaned data to '{output_path}'...\")\n",
    "    try:\n",
    "        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "            for sheet_name, df in cleaned_sheets.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        \n",
    "        print(f\"✅ Successfully saved cleaned data to '{output_path}'\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n📋 CLEANING SUMMARY:\")\n",
    "        print(\"=\"*60)\n",
    "        for sheet_name, df in cleaned_sheets.items():\n",
    "            print(f\"Sheet '{sheet_name}': {len(df)} products, {len(df.columns)} columns\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb8680b",
   "metadata": {},
   "source": [
    "# Cell 15: Execute the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caf05b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Sales Data Cleaning Pipeline\n",
      "============================================================\n",
      "📂 Loading Excel data...\n",
      "Successfully loaded 2 sheets: ['Private label', 'Brand']\n",
      "\n",
      "🔄 Starting cleaning pipeline for sheet: 'Private label'\n",
      "Initial data shape: (41, 43)\n",
      "\n",
      "1️⃣ Cleaning column names...\n",
      "Cleaned 43 column names\n",
      "\n",
      "2️⃣ Converting date columns...\n",
      "Found 0 date columns: []\n",
      "\n",
      "3️⃣ Handling missing values...\n",
      "Found 38 consistent products (>50% non-zero sales)\n",
      "Removed 0 products with all zero sales\n",
      "\n",
      "4️⃣ Applying rule-based validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n",
      "C:\\Users\\yash.bhosale\\AppData\\Local\\Temp\\ipykernel_36672\\1664708482.py:13: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.loc[idx, sales_cols] = df.loc[idx, sales_cols].interpolate(method='linear', limit_direction='both')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "match() missing 1 required positional argument: 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(file_path, output_path)\u001b[39m\n\u001b[32m     14\u001b[39m cleaned_sheets = {}\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sheet_name, df \u001b[38;5;129;01min\u001b[39;00m sheets.items():\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     cleaned_df = \u001b[43mclean_sales_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     cleaned_sheets[sheet_name] = cleaned_df\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Save cleaned data\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mclean_sales_data\u001b[39m\u001b[34m(df, sheet_name)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Step 4: Apply rule-based validation\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m4️⃣ Applying rule-based validation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m df = \u001b[43mrule_based_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Step 5: Detect outliers\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m5️⃣ Detecting outliers...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mrule_based_validation\u001b[39m\u001b[34m(df, max_threshold)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrule_based_validation\u001b[39m(df, max_threshold=\u001b[32m10\u001b[39m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply rule-based validation for negative and unrealistic sales\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     sales_cols = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m^\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[38;5;132;43;01m{3}\u001b[39;49;00m\u001b[33;43m-\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43md\u001b[39;49m\u001b[38;5;132;43;01m{2}\u001b[39;49;00m\u001b[33;43m$\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[32m      4\u001b[39m     validation_flags = pd.DataFrame(\u001b[32m0\u001b[39m, index=df.index, columns=[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_invalid\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m sales_cols])\n\u001b[32m      5\u001b[39m     invalid_count = \u001b[32m0\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: match() missing 1 required positional argument: 'string'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836cfb30",
   "metadata": {},
   "source": [
    "# Cell 16: Optional - Load and Explore Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cdc0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = load_excel_data('Cleaned_Sales_History_Updated.xlsx')\n",
    "\n",
    "if cleaned_data:\n",
    "    for sheet_name, df in cleaned_data.items():\n",
    "        print(f\"\\n📋 Summary for {sheet_name}:\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns[:10])}...\")  # Show first 10 columns\n",
    "        print(f\"Sample data:\")\n",
    "        print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b451656",
   "metadata": {},
   "source": [
    "# Cell 17: Optional - Custom Product Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d2b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_name = 'Private label'  # or 'Brand'\n",
    "product_id = 'YOUR_PRODUCT_ID'  # Replace with actual product ID\n",
    "\n",
    "if 'cleaned_data' in locals():\n",
    "    df = cleaned_data[sheet_name]\n",
    "    visualize_outliers(df, product_id, sheet_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
